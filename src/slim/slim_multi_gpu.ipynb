{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TF-Slim Walkthrough for multi-GPU setup\n",
    "\n",
    "This is an adaptation of the Flowers training example in slim_walkthrough.ipynb, but allows you to split the training across multiple GPUs.\n",
    "\n",
    "The tower loss and training loop code is taken from the CIFAR demo at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Defining properties of our system\n",
    "\n",
    "How many GPUs we have, batch size on each GPU, etc.\n",
    "\n",
    "These settings are for two GeForce GTX 1080 Graphics Cards. Depending on the number and spec of your graphics cards you will have to adjust NUM_GPUS and BATCH_SIZE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # How many images can pass through *a single GPU*\n",
    "                 # (if they are different specs you'll have to adapt the script)\n",
    "MAX_STEPS = 1000\n",
    "NUM_GPUS = 8\n",
    "url = \"http://download.tensorflow.org/data/flowers.tar.gz\"\n",
    "flowers_data_dir = '/tmp/flowers'\n",
    "dataset=flowers\n",
    "dataset_dir=flowers_data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preparing the data\n",
    "\n",
    "If you've already worked through slim_walkthrough.ipynb you will have this data on your system.\n",
    "\n",
    "### Download the Flowers Dataset\n",
    "<a id='DownloadFlowers'></a>\n",
    "\n",
    "We've made available a tarball of the Flowers dataset which has already been converted to TFRecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading flowers.tar.gz 100.0%\n",
      "Successfully downloaded flowers.tar.gz 228649660 bytes.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import dataset_utils\n",
    "\n",
    "\n",
    "\n",
    "if not tf.gfile.Exists(flowers_data_dir):\n",
    "    tf.gfile.MakeDirs(flowers_data_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, flowers_data_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Download the Inception V1 checkpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading inception_v1_2016_08_28.tar.gz 100.0%\n",
      "Successfully downloaded inception_v1_2016_08_28.tar.gz 24642554 bytes.\n"
     ]
    }
   ],
   "source": [
    "from datasets import dataset_utils\n",
    "\n",
    "url = \"http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\"\n",
    "checkpoints_dir = '/tmp/checkpoints'\n",
    "\n",
    "if not tf.gfile.Exists(checkpoints_dir):\n",
    "    tf.gfile.MakeDirs(checkpoints_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define the methods we'll need during training\n",
    "\n",
    "The batch method is slightly different from the single GPU case as we pass a data_provider which can give images as a stream, where the dataset has already been located on disk - so that we don't have two data_provider objects trying to access our Tfrecord files concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import inception_preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def load_batch(data_provider, batch_size=32, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      data_provider: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_raw, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training, fast_mode=True)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The loss is slightly different as we will need to call our loss function once for each GPU, so they will reside in different scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "def get_total_loss(scope, name=\"total_loss\"):\n",
    "    losses = slim.losses.get_losses(scope=scope)\n",
    "    losses += slim.losses.get_regularization_losses(scope=scope)\n",
    "    return math_ops.add_n(losses, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Every GPU instance is in an abstraction called a \"tower\" which has its own scope. So we have a function to get the loss for a given tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def tower_loss(scope, data_provider):\n",
    "    \"\"\"Calculate the total loss on a single tower running the model.\n",
    "\n",
    "    Args:\n",
    "    scope: unique prefix string identifying the tower, e.g. 'tower_0'\n",
    "\n",
    "    Returns:\n",
    "    Tensor of shape [] containing the total loss for a batch of data\n",
    "    \"\"\"    \n",
    "   \n",
    "    images, _, labels = load_batch(data_provider, batch_size=BATCH_SIZE, height=image_size, width=image_size, is_training=True)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "    \n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes, scope=scope)\n",
    "    \n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels, scope=scope)\n",
    "    \n",
    "    total_loss = get_total_loss(scope)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "When the losses have been calculated on each tower, we need to calculate the gradients for each tower too and average them before updating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "    is over individual gradients. The inner list is over the gradient\n",
    "    calculation for each tower.\n",
    "    Returns:\n",
    "    List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "    across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(0, grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Now fine-tune the model\n",
    "\n",
    "We will fine tune the inception model on the Flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "\n",
    "\n",
    "def get_init_fn():\n",
    "    \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "    checkpoint_exclude_scopes=[\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "    \n",
    "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "\n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "      os.path.join(checkpoints_dir, 'inception_v1.ckpt'),\n",
    "      variables_to_restore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_dir = '/tmp/inception_finetuned/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('number of classes: ', 5)\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:195 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:204 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:219 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From preprocessing/inception_preprocessing.py:231 in preprocess_for_train.: image_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.image. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, the max_images argument was renamed to max_outputs.\n",
      "WARNING:tensorflow:From <ipython-input-23-58c74685ce7e>:38 in <module>.: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "WARNING:tensorflow:From <ipython-input-23-58c74685ce7e>:41 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "WARNING:tensorflow:From <ipython-input-23-58c74685ce7e>:55 in <module>.: __init__ (from tensorflow.python.training.summary_io) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.FileWriter. The interface and behavior is the same; this is just a rename.\n",
      "2017-02-24 13:39:35.429843: step 0, loss = 1.89  (20.7 examples/sec; 1.547 sec/batch)\n",
      "2017-02-24 13:40:57.849356: step 10, loss = 2.08  (171.7 examples/sec; 0.186 sec/batch)\n",
      "2017-02-24 13:41:12.912261: step 20, loss = 1.86  (180.7 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:41:27.503347: step 30, loss = 2.03  (171.9 examples/sec; 0.186 sec/batch)\n",
      "2017-02-24 13:41:42.125303: step 40, loss = 1.75  (176.2 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 13:41:56.792258: step 50, loss = 1.59  (182.9 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 13:42:11.091967: step 60, loss = 1.70  (185.9 examples/sec; 0.172 sec/batch)\n",
      "2017-02-24 13:42:25.454976: step 70, loss = 1.51  (158.7 examples/sec; 0.202 sec/batch)\n",
      "2017-02-24 13:42:40.104116: step 80, loss = 1.44  (187.4 examples/sec; 0.171 sec/batch)\n",
      "2017-02-24 13:42:54.289997: step 90, loss = 1.57  (179.9 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 13:43:08.679862: step 100, loss = 1.42  (180.0 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 13:43:22.983285: step 110, loss = 1.36  (181.0 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:43:37.463147: step 120, loss = 1.28  (177.6 examples/sec; 0.180 sec/batch)\n",
      "2017-02-24 13:43:51.618483: step 130, loss = 1.39  (181.2 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:44:06.227047: step 140, loss = 1.16  (175.9 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 13:44:20.462979: step 150, loss = 1.27  (184.5 examples/sec; 0.173 sec/batch)\n",
      "2017-02-24 13:44:34.937855: step 160, loss = 1.10  (184.8 examples/sec; 0.173 sec/batch)\n",
      "2017-02-24 13:44:49.678629: step 170, loss = 1.25  (174.1 examples/sec; 0.184 sec/batch)\n",
      "2017-02-24 13:45:03.872770: step 180, loss = 1.28  (182.5 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 13:45:18.549816: step 190, loss = 1.19  (163.1 examples/sec; 0.196 sec/batch)\n",
      "2017-02-24 13:45:32.831088: step 200, loss = 1.16  (177.2 examples/sec; 0.181 sec/batch)\n",
      "2017-02-24 13:45:46.950733: step 210, loss = 1.09  (177.5 examples/sec; 0.180 sec/batch)\n",
      "2017-02-24 13:46:01.175179: step 220, loss = 1.13  (184.3 examples/sec; 0.174 sec/batch)\n",
      "2017-02-24 13:46:15.467603: step 230, loss = 0.95  (175.0 examples/sec; 0.183 sec/batch)\n",
      "2017-02-24 13:46:29.530122: step 240, loss = 1.04  (185.8 examples/sec; 0.172 sec/batch)\n",
      "2017-02-24 13:46:44.013642: step 250, loss = 1.03  (170.0 examples/sec; 0.188 sec/batch)\n",
      "2017-02-24 13:46:58.007871: step 260, loss = 0.95  (181.7 examples/sec; 0.176 sec/batch)\n",
      "2017-02-24 13:47:12.106286: step 270, loss = 1.17  (175.9 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 13:47:26.435207: step 280, loss = 1.03  (179.3 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 13:47:40.679141: step 290, loss = 0.95  (190.3 examples/sec; 0.168 sec/batch)\n",
      "2017-02-24 13:47:55.019509: step 300, loss = 0.93  (179.1 examples/sec; 0.179 sec/batch)\n",
      "2017-02-24 13:48:09.184900: step 310, loss = 1.04  (177.5 examples/sec; 0.180 sec/batch)\n",
      "2017-02-24 13:48:23.524016: step 320, loss = 1.19  (176.8 examples/sec; 0.181 sec/batch)\n",
      "2017-02-24 13:48:37.643662: step 330, loss = 1.07  (181.8 examples/sec; 0.176 sec/batch)\n",
      "2017-02-24 13:48:52.156455: step 340, loss = 0.91  (183.3 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 13:49:06.739945: step 350, loss = 0.80  (177.1 examples/sec; 0.181 sec/batch)\n",
      "2017-02-24 13:49:21.542076: step 360, loss = 0.86  (156.5 examples/sec; 0.205 sec/batch)\n",
      "2017-02-24 13:49:35.709111: step 370, loss = 0.91  (180.6 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:49:49.865637: step 380, loss = 0.94  (191.8 examples/sec; 0.167 sec/batch)\n",
      "2017-02-24 13:50:04.267768: step 390, loss = 0.96  (180.7 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:50:18.518777: step 400, loss = 0.81  (188.9 examples/sec; 0.169 sec/batch)\n",
      "2017-02-24 13:50:32.788036: step 410, loss = 0.82  (180.5 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:50:47.117351: step 420, loss = 0.85  (182.4 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 13:51:01.396652: step 430, loss = 0.71  (183.8 examples/sec; 0.174 sec/batch)\n",
      "2017-02-24 13:51:15.509875: step 440, loss = 0.78  (185.1 examples/sec; 0.173 sec/batch)\n",
      "2017-02-24 13:51:29.713723: step 450, loss = 0.79  (180.7 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:51:43.919065: step 460, loss = 0.83  (179.0 examples/sec; 0.179 sec/batch)\n",
      "2017-02-24 13:51:58.326775: step 470, loss = 0.85  (180.7 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:52:12.766004: step 480, loss = 0.67  (178.3 examples/sec; 0.179 sec/batch)\n",
      "2017-02-24 13:52:26.953364: step 490, loss = 0.76  (183.0 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 13:52:41.287826: step 500, loss = 0.72  (175.8 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 13:52:55.611214: step 510, loss = 0.67  (181.0 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:53:09.845023: step 520, loss = 0.71  (172.5 examples/sec; 0.185 sec/batch)\n",
      "2017-02-24 13:53:24.160329: step 530, loss = 0.88  (178.8 examples/sec; 0.179 sec/batch)\n",
      "2017-02-24 13:53:38.411710: step 540, loss = 0.89  (170.7 examples/sec; 0.188 sec/batch)\n",
      "2017-02-24 13:53:52.663857: step 550, loss = 0.91  (187.2 examples/sec; 0.171 sec/batch)\n",
      "2017-02-24 13:54:07.087361: step 560, loss = 0.68  (177.8 examples/sec; 0.180 sec/batch)\n",
      "2017-02-24 13:54:21.549851: step 570, loss = 0.69  (181.1 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:54:35.615128: step 580, loss = 0.66  (177.5 examples/sec; 0.180 sec/batch)\n",
      "2017-02-24 13:54:50.290891: step 590, loss = 0.67  (173.8 examples/sec; 0.184 sec/batch)\n",
      "2017-02-24 13:55:04.816963: step 600, loss = 0.81  (181.3 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:55:18.801479: step 610, loss = 0.72  (179.5 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 13:55:33.174455: step 620, loss = 0.71  (178.6 examples/sec; 0.179 sec/batch)\n",
      "2017-02-24 13:55:47.548117: step 630, loss = 0.71  (154.4 examples/sec; 0.207 sec/batch)\n",
      "2017-02-24 13:56:02.017127: step 640, loss = 0.69  (181.7 examples/sec; 0.176 sec/batch)\n",
      "2017-02-24 13:56:16.197176: step 650, loss = 0.92  (182.4 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 13:56:30.335846: step 660, loss = 0.73  (181.3 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:56:44.515801: step 670, loss = 0.53  (177.4 examples/sec; 0.180 sec/batch)\n",
      "2017-02-24 13:56:58.723504: step 680, loss = 0.75  (179.3 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 13:57:13.117249: step 690, loss = 0.54  (179.7 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 13:57:27.298370: step 700, loss = 0.79  (189.0 examples/sec; 0.169 sec/batch)\n",
      "2017-02-24 13:57:41.873063: step 710, loss = 0.58  (165.5 examples/sec; 0.193 sec/batch)\n",
      "2017-02-24 13:57:56.126145: step 720, loss = 0.79  (181.9 examples/sec; 0.176 sec/batch)\n",
      "2017-02-24 13:58:10.608442: step 730, loss = 0.58  (182.6 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 13:58:24.638116: step 740, loss = 0.53  (188.8 examples/sec; 0.169 sec/batch)\n",
      "2017-02-24 13:58:38.861305: step 750, loss = 0.73  (181.1 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:58:53.103209: step 760, loss = 0.61  (180.5 examples/sec; 0.177 sec/batch)\n",
      "2017-02-24 13:59:07.259862: step 770, loss = 0.61  (188.8 examples/sec; 0.169 sec/batch)\n",
      "2017-02-24 13:59:21.504013: step 780, loss = 0.69  (179.7 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 13:59:36.038524: step 790, loss = 0.81  (174.2 examples/sec; 0.184 sec/batch)\n",
      "2017-02-24 13:59:50.563671: step 800, loss = 0.54  (176.9 examples/sec; 0.181 sec/batch)\n",
      "2017-02-24 14:00:05.098641: step 810, loss = 0.66  (179.7 examples/sec; 0.178 sec/batch)\n",
      "2017-02-24 14:00:19.294072: step 820, loss = 0.73  (181.8 examples/sec; 0.176 sec/batch)\n",
      "2017-02-24 14:00:33.752352: step 830, loss = 0.66  (183.8 examples/sec; 0.174 sec/batch)\n",
      "2017-02-24 14:00:47.823655: step 840, loss = 0.49  (183.0 examples/sec; 0.175 sec/batch)\n",
      "2017-02-24 14:01:02.355655: step 850, loss = 0.80  (176.4 examples/sec; 0.181 sec/batch)\n",
      "2017-02-24 14:01:16.778810: step 860, loss = 0.64  (175.7 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 14:01:31.019168: step 870, loss = 0.55  (158.3 examples/sec; 0.202 sec/batch)\n",
      "2017-02-24 14:01:45.237832: step 880, loss = 0.71  (187.1 examples/sec; 0.171 sec/batch)\n",
      "2017-02-24 14:02:00.008904: step 890, loss = 0.50  (160.4 examples/sec; 0.199 sec/batch)\n",
      "2017-02-24 14:02:14.594342: step 900, loss = 0.76  (175.8 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 14:02:28.631399: step 910, loss = 0.67  (175.7 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 14:02:42.952188: step 920, loss = 0.53  (185.5 examples/sec; 0.173 sec/batch)\n",
      "2017-02-24 14:02:57.435007: step 930, loss = 0.67  (173.9 examples/sec; 0.184 sec/batch)\n",
      "2017-02-24 14:03:12.008889: step 940, loss = 0.55  (173.7 examples/sec; 0.184 sec/batch)\n",
      "2017-02-24 14:03:26.258822: step 950, loss = 0.83  (182.3 examples/sec; 0.176 sec/batch)\n",
      "2017-02-24 14:03:40.686576: step 960, loss = 0.51  (157.8 examples/sec; 0.203 sec/batch)\n",
      "2017-02-24 14:03:54.732831: step 970, loss = 0.62  (184.3 examples/sec; 0.174 sec/batch)\n",
      "2017-02-24 14:04:08.962232: step 980, loss = 0.61  (175.9 examples/sec; 0.182 sec/batch)\n",
      "2017-02-24 14:04:23.427557: step 990, loss = 0.48  (185.7 examples/sec; 0.172 sec/batch)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'final_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-58c74685ce7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Finished training. Last batch loss %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfinal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'final_loss' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    global_step = tf.get_variable(\"global_step\", [], initializer=tf.constant_initializer(0),\n",
    "                                 trainable=False)\n",
    "    \n",
    "    # Specify the optimizer and create the train op:    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # Here you can substitute the flowers dataset for your own dataset.\n",
    "    dataset = flowers.get_split('train', flowers_data_dir)    \n",
    "    print (\"number of classes: \", dataset.num_classes)\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8, shuffle=True)\n",
    "    \n",
    "    \n",
    "    tower_grads = []\n",
    "    losses = []\n",
    "    for i in range(NUM_GPUS):\n",
    "        with tf.device(\"/gpu:\" + str(i)):\n",
    "            with tf.name_scope(\"tower_\" + str(i)) as scope:\n",
    "                loss = tower_loss(scope, data_provider)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                grads = optimizer.compute_gradients(loss)\n",
    "                \n",
    "                tower_grads.append(grads)\n",
    "                \n",
    "    grads = average_gradients(tower_grads)\n",
    "    \n",
    "    apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "    \n",
    "    train_op = apply_gradient_op\n",
    "    \n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    \n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False))\n",
    "    sess.run(init)\n",
    "    \n",
    "    init_fn = get_init_fn()\n",
    "    init_fn(sess)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # This code gets the average loss, and the losses on GPUs 1 and 2, to print.\n",
    "        # If you have more GPUs then you will need to adapt it.\n",
    "        _, loss_value = sess.run([train_op, loss])\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            num_examples_per_step = BATCH_SIZE * NUM_GPUS\n",
    "            examples_per_sec = num_examples_per_step / duration\n",
    "            sec_per_batch = float(duration) / NUM_GPUS\n",
    "\n",
    "            format_str = ('%s: step %d, loss = %.2f  (%.1f examples/sec; %.3f '\n",
    "                          'sec/batch)')\n",
    "            print (format_str % (datetime.now(), step, loss_value, \n",
    "                                 examples_per_sec, sec_per_batch))\n",
    "\n",
    "        # Save the model checkpoint periodically.\n",
    "        if step % 1000 == 0 or (step + 1) == MAX_STEPS:\n",
    "            checkpoint_path = os.path.join(train_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step=step)\n",
    "        \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Apply fine tuned model to some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Dataset' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-6fc826b2b2e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflowers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflowers_data_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Create the model, use the default arg scope to configure the batch norm parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ff6311bc932b>\u001b[0m in \u001b[0;36mload_batch\u001b[0;34m(data_provider, batch_size, height, width, is_training)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \"\"\"\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimage_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Preprocess image for usage by Inception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dataset' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "batch_size = 3\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    images, images_raw, labels = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.initialize_local_variables())\n",
    "            init_fn(sess)\n",
    "            np_probabilities, np_images_raw, np_labels = sess.run([probabilities, images_raw, labels])\n",
    "    \n",
    "            for i in xrange(batch_size): \n",
    "                image = np_images_raw[i, :, :, :]\n",
    "                true_label = np_labels[i]\n",
    "                predicted_label = np.argmax(np_probabilities[i, :])\n",
    "                predicted_name = dataset.labels_to_names[predicted_label]\n",
    "                true_name = dataset.labels_to_names[true_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(image.astype(np.uint8))\n",
    "                plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
    "                plt.axis('off')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
