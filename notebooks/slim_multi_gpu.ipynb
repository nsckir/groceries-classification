{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-Slim Walkthrough for multi-GPU setup\n",
    "\n",
    "This is an adaptation of the Flowers training example in slim_walkthrough.ipynb, but allows you to split the training across multiple GPUs.\n",
    "\n",
    "The tower loss and training loop code is taken from the CIFAR demo at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining properties of our system\n",
    "\n",
    "How many GPUs we have, batch size on each GPU, etc.\n",
    "\n",
    "These settings are for two GeForce GTX 1080 Graphics Cards. Depending on the number and spec of your graphics cards you will have to adjust NUM_GPUS and BATCH_SIZE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 158 # How many images can pass through *a single GPU*\n",
    "                 # (if they are different specs you'll have to adapt the script)\n",
    "MAX_STEPS = 1000000\n",
    "NUM_GPUS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "If you've already worked through slim_walkthrough.ipynb you will have this data on your system.\n",
    "\n",
    "### Download the Flowers Dataset\n",
    "<a id='DownloadFlowers'></a>\n",
    "\n",
    "We've made available a tarball of the Flowers dataset which has already been converted to TFRecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datasets import dataset_utils\n",
    "\n",
    "url = \"http://download.tensorflow.org/data/flowers.tar.gz\"\n",
    "flowers_data_dir = '/tmp/flowers'\n",
    "\n",
    "if not tf.gfile.Exists(flowers_data_dir):\n",
    "    tf.gfile.MakeDirs(flowers_data_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, flowers_data_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Inception V1 checkpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import dataset_utils\n",
    "\n",
    "url = \"http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\"\n",
    "checkpoints_dir = '/tmp/checkpoints'\n",
    "\n",
    "if not tf.gfile.Exists(checkpoints_dir):\n",
    "    tf.gfile.MakeDirs(checkpoints_dir)\n",
    "\n",
    "dataset_utils.download_and_uncompress_tarball(url, checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the methods we'll need during training\n",
    "\n",
    "The batch method is slightly different from the single GPU case as we pass a data_provider which can give images as a stream, where the dataset has already been located on disk - so that we don't have two data_provider objects trying to access our Tfrecord files concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from preprocessing import inception_preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "\n",
    "def load_batch(data_provider, batch_size=32, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      data_provider: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    image_raw, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training, fast_mode=True)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is slightly different as we will need to call our loss function once for each GPU, so they will reside in different scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import math_ops\n",
    "\n",
    "def get_total_loss(scope, name=\"total_loss\"):\n",
    "    losses = slim.losses.get_losses(scope=scope)\n",
    "    losses += slim.losses.get_regularization_losses(scope=scope)\n",
    "    return math_ops.add_n(losses, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every GPU instance is in an abstraction called a \"tower\" which has its own scope. So we have a function to get the loss for a given tower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tower_loss(scope, data_provider):\n",
    "    \"\"\"Calculate the total loss on a single tower running the model.\n",
    "\n",
    "    Args:\n",
    "    scope: unique prefix string identifying the tower, e.g. 'tower_0'\n",
    "\n",
    "    Returns:\n",
    "    Tensor of shape [] containing the total loss for a batch of data\n",
    "    \"\"\"    \n",
    "   \n",
    "    images, _, labels = load_batch(data_provider, batch_size=BATCH_SIZE, height=image_size, width=image_size, is_training=True)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "    \n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes, scope=scope)\n",
    "    \n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels, scope=scope)\n",
    "    \n",
    "    total_loss = get_total_loss(scope)\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the losses have been calculated on each tower, we need to calculate the gradients for each tower too and average them before updating our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "\n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "    is over individual gradients. The inner list is over the gradient\n",
    "    calculation for each tower.\n",
    "    Returns:\n",
    "    List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "    across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(0, grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now fine-tune the model\n",
    "\n",
    "We will fine tune the inception model on the Flowers dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "from preprocessing import inception_preprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "\n",
    "\n",
    "def get_init_fn():\n",
    "    \"\"\"Returns a function run by the chief worker to warm-start the training.\"\"\"\n",
    "    checkpoint_exclude_scopes=[\"InceptionV1/Logits\", \"InceptionV1/AuxLogits\"]\n",
    "    \n",
    "    exclusions = [scope.strip() for scope in checkpoint_exclude_scopes]\n",
    "\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "\n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "      os.path.join(checkpoints_dir, 'inception_v1.ckpt'),\n",
    "      variables_to_restore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir = '/tmp/inception_finetuned/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    global_step = tf.get_variable(\"global_step\", [], initializer=tf.constant_initializer(0),\n",
    "                                 trainable=False)\n",
    "    \n",
    "    # Specify the optimizer and create the train op:    \n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001)\n",
    "    \n",
    "    # Here you can substitute the flowers dataset for your own dataset.\n",
    "    dataset = flowers.get_split('train', flowers_data_dir)    \n",
    "    print (\"number of classes: \", dataset.num_classes)\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8, shuffle=True)\n",
    "    \n",
    "    \n",
    "    tower_grads = []\n",
    "    losses = []\n",
    "    for i in range(NUM_GPUS):\n",
    "        with tf.device(\"/gpu:\" + str(i)):\n",
    "            with tf.name_scope(\"tower_\" + str(i)) as scope:\n",
    "                loss = tower_loss(scope, data_provider)\n",
    "                losses.append(loss)\n",
    "                \n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "                \n",
    "                grads = optimizer.compute_gradients(loss)\n",
    "                \n",
    "                tower_grads.append(grads)\n",
    "                \n",
    "    grads = average_gradients(tower_grads)\n",
    "    \n",
    "    apply_gradient_op = optimizer.apply_gradients(grads, global_step=global_step)\n",
    "    \n",
    "    train_op = apply_gradient_op\n",
    "    \n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    \n",
    "    # Build an initialization operation to run below.\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Start running operations on the Graph.\n",
    "    sess = tf.Session(config=tf.ConfigProto(\n",
    "        allow_soft_placement=True,\n",
    "        log_device_placement=False))\n",
    "    sess.run(init)\n",
    "    \n",
    "    init_fn = get_init_fn()\n",
    "    init_fn(sess)\n",
    "\n",
    "    # Start the queue runners.\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "\n",
    "    summary_writer = tf.train.SummaryWriter(train_dir, sess.graph)\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # This code gets the average loss, and the losses on GPUs 1 and 2, to print.\n",
    "        # If you have more GPUs then you will need to adapt it.\n",
    "        _, loss_value, losses_value_0, losses_value_1 = sess.run([train_op, loss, losses[0], losses[1]])\n",
    "        duration = time.time() - start_time\n",
    "\n",
    "        assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            num_examples_per_step = BATCH_SIZE * NUM_GPUS\n",
    "            examples_per_sec = num_examples_per_step / duration\n",
    "            sec_per_batch = float(duration) / NUM_GPUS\n",
    "\n",
    "            format_str = ('%s: step %d, loss = %.2f (0: %.2f, 1: %.2f) (%.1f examples/sec; %.3f '\n",
    "                          'sec/batch)')\n",
    "            print (format_str % (datetime.now(), step, loss_value, losses_value_0, losses_value_1,\n",
    "                                 examples_per_sec, sec_per_batch))\n",
    "\n",
    "        # Save the model checkpoint periodically.\n",
    "        if step % 1000 == 0 or (step + 1) == MAX_STEPS:\n",
    "            checkpoint_path = os.path.join(train_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step=step)\n",
    "        \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply fine tuned model to some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import flowers\n",
    "from nets import inception\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "batch_size = 3\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = flowers.get_split('train', flowers_data_dir)\n",
    "    images, images_raw, labels = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.initialize_local_variables())\n",
    "            init_fn(sess)\n",
    "            np_probabilities, np_images_raw, np_labels = sess.run([probabilities, images_raw, labels])\n",
    "    \n",
    "            for i in xrange(batch_size): \n",
    "                image = np_images_raw[i, :, :, :]\n",
    "                true_label = np_labels[i]\n",
    "                predicted_label = np.argmax(np_probabilities[i, :])\n",
    "                predicted_name = dataset.labels_to_names[predicted_label]\n",
    "                true_name = dataset.labels_to_names[true_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(image.astype(np.uint8))\n",
    "                plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
    "                plt.axis('off')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
